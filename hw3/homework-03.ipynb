{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее Задание 3.\n",
    "Дедлайн - 24 мая 23:59 по МСК (GMT+3).\n",
    "\n",
    "Форма сдачи - jupyter notebook. \n",
    "Сдавать в [классрум](https://classroom.google.com/c/NjYxNjY4MjY3NDIw?cjc=pho754c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "matplotlib.rcParams['image.cmap'] = 'jet'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 1.\n",
    "Рассмотрим две функции $f_0, f_1$ заданные на отрезке $[-2\\pi, 2\\pi]$.\n",
    "$$\n",
    "f_0(x) = x + 0.95 \\sin(x)\n",
    "$$\n",
    "$$\n",
    "f_1(x) = \\frac{\\cos(1/2 \\pi + x / 5)}{\\sin^2(1/2 \\pi + x / 5)}\n",
    "$$\n",
    "Код для вычисления значений функций $f_0$ и $f_1$ приведен ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_function_0(x):\n",
    "    return x + 0.95 * np.sin(x)\n",
    "\n",
    "\n",
    "def compute_function_1(x):\n",
    "    t = np.pi * (0.5 + x / 5.0 / np.pi)\n",
    "    return np.cos(t) / np.sin(t) / np.sin(t)\n",
    "\n",
    "\n",
    "def make_plots_problem_1():\n",
    "    numx = 10001\n",
    "    x = np.linspace(-2.0 * np.pi, 2.0 * np.pi, numx)\n",
    "    y0 = compute_function_0(x)\n",
    "    y1 = compute_function_1(x)\n",
    "    plt.figure()\n",
    "    plt.plot(x, y0, c='r', label='$f_0(x)$')\n",
    "    plt.plot(x, y1, c='b', label='$f_1(x)$')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_plots_problem_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя метод деления пополам найдите корни уравнений;\n",
    "$$\n",
    "f_0(x) = 4\n",
    "$$\n",
    "$$\n",
    "f_1(x) = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(left_bound, right_bound, target, func, eps=1.0e-10):\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "    pass\n",
    "\n",
    "\n",
    "def solve_problem_1():\n",
    "    target = 4.0\n",
    "\n",
    "    x_0 = binary_search(...)  # solution for f_0\n",
    "    x_1 = binary_search(...)  # solution for f_1\n",
    "    assert abs(compute_function_0(x_0) - target) < 1e-10\n",
    "    assert abs(compute_function_1(x_1) - target) < 1e-10\n",
    "\n",
    "\n",
    "solve_problem_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 2.\n",
    "Рассмотрим движение космического объекта в поле тяжести звезды.\n",
    "Траектория движения объект может вычислена по уравнениям движения, если заданы начальные координаты и скорость.\n",
    "\n",
    "Предполагается, что в начальный момент времени $t_0 = 0$ объект находится в точке $[1.0, 0.0]$.\n",
    "Задача состоит в том, чтобы вычислить скорость, которую должен иметь объект, чтобы в момент времени $t_1 = 2$ оказаться в точке с координатами $[-0.8, 0.3]$.\n",
    "\n",
    "В данном случае, положение в момент времени $t_1 = 2$ - функция начальной скорости $v$:\n",
    "$x_{\\text{final}} = f(v)$\n",
    "\n",
    "Решите данную задачу численно, используя метод Ньютона.\n",
    "\n",
    "Функция, которые вычисляют координаты объекта по начальной скорости, а так же матрицу Якоби приведены ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ode_solver import compute_numerical_solution_rk_5, compute_numerical_trajectory_rk_5\n",
    "\n",
    "\n",
    "def ffun(v, t_final=2.0):\n",
    "    \"\"\"\n",
    "    This function computes the position by velocity vector\n",
    "    \"\"\"\n",
    "    t0 = 0.0\n",
    "    x = np.zeros(4)\n",
    "    x[0] = 1.0\n",
    "    x[2:] = v.copy()\n",
    "    nstep = 1000\n",
    "    return compute_numerical_solution_rk_5(t0, x, t_final, nstep)[:-2]\n",
    "\n",
    "\n",
    "def initial_guess_problem_2(x_final, t_final=2.0):\n",
    "    x_start = np.asarray([1.0, 0.0])\n",
    "    return (x_final - x_start) / t_final\n",
    "\n",
    "\n",
    "def compute_jacoby_matrix_problem_2(x):\n",
    "    dx = 1.0e-6\n",
    "    dimx = x.size\n",
    "    jmat = np.zeros((dimx, dimx))\n",
    "    f0 = ffun(x)\n",
    "    for idx in range(dimx):\n",
    "        x_pert = x.copy()\n",
    "        x_pert[idx] += dx\n",
    "        jmat[:, idx] = (ffun(x_pert) - f0) / dx\n",
    "    return jmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите траекторию объекта, и отметьте начанльные и конечные точки на траектории.\n",
    "Траекорию объекта можно вычислить при помощи функции\n",
    "`compute_trajectory_problem_2(v, t_final=2.0, nstep=1000)`.\n",
    "Смотрите пример ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_trajectory_problem_2(v, t_final=2.0, nstep=1000):\n",
    "    x_start = np.asarray([1.0, 0.0])\n",
    "    x = np.zeros(4)\n",
    "    x[:2] = x_start.copy()\n",
    "    x[2:] = v.copy()\n",
    "    x_numer = compute_numerical_trajectory_rk_5(0.0, x, t_final, nstep)\n",
    "    return x_numer[:, :2]\n",
    "\n",
    "\n",
    "def make_plots_problem_2():\n",
    "    v = np.random.normal(0.0, 1.0, 2)\n",
    "    print(f'Velocity: ({v[0]}, {v[1]})')\n",
    "    x_trajectory = compute_trajectory_problem_2(v, t_final=2.0, nstep=1000)\n",
    "\n",
    "    plt.plot(x_trajectory[:, 0], x_trajectory[:, 1])\n",
    "    plt.xlabel(r'$X_0$')\n",
    "    plt.ylabel(r'$X_1$')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_plots_problem_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_solver(y, x0, num_iter=400):\n",
    "    \"\"\"\n",
    "    Newton method\n",
    "    You can use np.linalg.solve() function\n",
    "    \"\"\"\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "    pass\n",
    "\n",
    "\n",
    "def solve_problem_2():\n",
    "    x_final = np.array([-0.8, 0.3])\n",
    "    v = initial_guess_problem_2(x_final)\n",
    "    v = newton_solver(x_final, v)\n",
    "\n",
    "    x_final_pred = ffun(v)\n",
    "    print(f'Start velocity: ({v[0]}, {v[1]})')\n",
    "    print(f'Final point: ({x_final_pred[0]}, {x_final_pred[1]})')\n",
    "    assert np.max(np.absolute(x_final - x_final_pred)) < 1.0e-2\n",
    "\n",
    "    x_trajectory = compute_trajectory_problem_2(v, t_final=2.0, nstep=1000)\n",
    "    plt.plot(x_trajectory[:, 0], x_trajectory[:, 1])\n",
    "    plt.xlabel(r'$X_0$')\n",
    "    plt.ylabel(r'$X_1$')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "solve_problem_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 3.\n",
    "\n",
    "Рассмотрим задачу линейной регрессии, которая описывается следующей функцией потерь:\n",
    "$$\n",
    "\\mathcal{L}(c) = \\frac{1}{2 N} \\sum_{i=1}^{N} \\left(y_i - \\sum_{\\alpha}c_a \\psi_a(x_i) \\right)^2 + \\alpha ||c||_1\n",
    "$$\n",
    "Здесь $\\psi_a(x)$ - полиномиальные функции от $x$, $y_i$ - значения функции в точках $x_i$, $c$ - вектор коэффициентов.\n",
    "\n",
    "Вектор коэффициентов $c$ вычисляется как точка минимума функции потерь:\n",
    "$$\n",
    "c = \\arg\\min(\\mathcal{L})\n",
    "$$\n",
    "\n",
    "Вычислите вектор коэффициентов, используя метод покоординатного спуска (Coordinate Descent Method) с циклическим порядком выбора координаты, по которой происходит минимизация.\n",
    "\n",
    "Зазача регрессии может быть инициализирована скриптом ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_poly_norm(poly_degree):\n",
    "    log_factor = 0.5 * np.log(2.0 * np.pi)\n",
    "    if poly_degree > 1:\n",
    "        log_factor += np.sum(np.log(np.linspace(1.0, poly_degree, poly_degree)))\n",
    "    return np.exp(0.5 * log_factor)\n",
    "\n",
    "\n",
    "def make_regression_problem_3(npoly, ndata, top_fraction=0.3, noise_std=0.01):\n",
    "    poly_coef = np.random.normal(0.0, 1.0, npoly)\n",
    "    xdata = np.random.normal(0.0, 1.0, ndata)\n",
    "    threshold = np.quantile(np.absolute(poly_coef), 1.0 - top_fraction)\n",
    "    for idx in range(poly_coef.size):\n",
    "        if np.absolute(poly_coef[idx]) < threshold:\n",
    "            poly_coef[idx] = 0.0\n",
    "    feature_matrix = np.zeros((ndata, npoly))\n",
    "    for idx in range(npoly):\n",
    "        feature_matrix[:, idx] = scipy.special.eval_hermitenorm(idx, xdata) / compute_poly_norm(idx)\n",
    "    ydata = np.dot(feature_matrix, poly_coef)\n",
    "    ydata += noise_std * np.random.normal(0.0, 1.0, ndata)\n",
    "    return feature_matrix, ydata, poly_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateDescent:\n",
    "    def __init__(self, alpha=1.0e-6, num_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, xdata, ydata):\n",
    "        # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "        pass\n",
    "\n",
    "    def predict(self, xdata):\n",
    "        # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_problem_3():\n",
    "    npoly = 11\n",
    "    ndata = 500\n",
    "    xdata, ydata, cdata = make_regression_problem_3(npoly, ndata, top_fraction=0.2, noise_std=0.001)\n",
    "    print(cdata)\n",
    "    baseline_model = linear_model.Lasso(alpha=1.0e-6, fit_intercept=False, tol=1.0e-8, max_iter=100000)\n",
    "    baseline_model.fit(xdata, ydata)\n",
    "    y_baseline_pred = baseline_model.predict(xdata)\n",
    "\n",
    "    custom_model = CoordinateDescent()\n",
    "    custom_model.fit(xdata, ydata)\n",
    "    y_custom_pred = custom_model.predict(xdata)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "    ax[0].scatter(ydata, y_baseline_pred, label='baseline model vs ground truth')\n",
    "    ax[0].scatter(ydata, y_custom_pred, label='custom model vs ground truth')\n",
    "    ax[0].set_title('Target vs baseline and custom model prediction')\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].scatter(cdata, baseline_model.coef_, label='baseline model vs ground truth')\n",
    "    ax[1].scatter(cdata, custom_model.coef_, label='custom model vs ground truth')\n",
    "    ax[1].set_title('Coefficient vs baseline and custom model coefficient')\n",
    "    ax[1].grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Baseline model inaccuracy: {np.max(np.absolute(cdata - baseline_model.coef_))}')\n",
    "    print(f'Custom model inaccuracy: {np.max(np.absolute(cdata - custom_model.coef_))}')\n",
    "\n",
    "\n",
    "make_plot_problem_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните результаты работы Вашего алгоритма с решением,\n",
    "которое выдает библиотека scikit-learn, использующая тот же алгоритм.\n",
    "\n",
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 4.\n",
    "Для вычисления таких параметров, как шум в данных, можно воспользоваться методом максимизации правдоподобия. Идея состоит в том, чтобы воспользоваться формулой Байеса:\n",
    "$$\n",
    "p_{\\text{likelihood}}(y | c, \\sigma) p_{\\text{prior}}(c) = p_{\\text{posterior}}(c | y, \\sigma) p_{\\text{evidence}}(y, \\sigma)\n",
    "$$\n",
    "Здесь функция правдоподобия $p_{\\text{likelihood}}(y | c, \\sigma)$ и априорное распределение параметров модели — нормальные распределения со своими стандартными отклонениями. $\\sigma$ — стандартное отклонение нормального распределения в функции правдоподобия.\n",
    "\n",
    "\n",
    "Таким образом, $p_{\\text{evidence}}(y, \\sigma)$ может быть найден интегрированием:\n",
    "$$\n",
    "p_{\\text{evidence}}(y, \\sigma) = \\int p_{\\text{likelihood}}(y | c, \\sigma) p_{\\text{prior}}(c) dc\n",
    "$$\n",
    "\n",
    "$p_{\\text{evidence}}(y, \\sigma)$ — описывает вероятность того, что вектор $y$ может быть получен в результате сбора данных. Соответственно, предлагается искать численно максимум $p_{\\text{evidence}}(y, \\sigma)$ как функцию $\\sigma$.\n",
    "\n",
    "Более того, достаточно найти максимум $\\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)$.\n",
    "\n",
    "Это можно сделать методом градиентного подъема. Для этого достаточно уметь вычислять:\n",
    "$$\n",
    "\\frac{\\partial \\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)}{\\partial \\sigma}\n",
    "$$\n",
    "\n",
    "Таким образом, можно найти оптимальную $\\sigma$ методом градиентного подъема.\n",
    "\n",
    "Для удобства (чтобы отдельно не обрабатывать случай $\\sigma < 0$) вводится следующая параметризация:\n",
    "\n",
    "$$\n",
    "\\sigma(s) = \\exp(s)\n",
    "$$\n",
    "\n",
    "Таким образом, можно вычислить производную $\\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)$\n",
    "по параметру $s$:\n",
    "$$\n",
    "\\frac{\\partial \\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)}{\\partial s} = \\frac{\\partial \\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)}{\\partial \\sigma} \\frac{\\partial \\sigma} {\\partial s} = \\frac{\\partial \\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)}{\\partial \\sigma} \\sigma(s)\n",
    "$$\n",
    "\n",
    "К сожаления, $\\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)$ не всегда может быть вычислен точно: зачастую данная величина вычисляется с точностью до некоторой случайной величины.\n",
    "По этой причине, необходимо воспользоваться методом стохастического градиентного подъема:\n",
    "$$\n",
    "s_{k + 1} = s_{k} + \\frac{\\Delta t}{(1 + k) ^ \\alpha} \\exp(s_{k})g_k\n",
    "$$\n",
    "Здесь $g_k$ — аппроксимация производной $\\log\\left(p_{\\text{evidence}}(y, \\sigma)\\right)$. $\\Delta t > 0$ и $\\alpha \\in [0.5, 1.0]$ — настраиваемые параметры.\n",
    "\n",
    "Подберите $\\alpha$ и $\\Delta t$ и вычислите оптимальное значение $s$ и $\\sigma$. Сравните с `noise_std` в коже ниже (приведена функция для вычисления $g_k$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_problem_4(npoly, ndata, top_fraction=0.3, noise_std=0.01):\n",
    "    poly_coef = np.random.normal(0.0, 1.0, npoly)\n",
    "    xdata = np.random.normal(0.0, 1.0, ndata)\n",
    "    threshold = np.quantile(np.absolute(poly_coef), 1.0 - top_fraction)\n",
    "    for idx in range(poly_coef.size):\n",
    "        if np.absolute(poly_coef[idx]) < threshold:\n",
    "            poly_coef[idx] = 0.0\n",
    "    feature_matrix = np.zeros((ndata, npoly))\n",
    "    for idx in range(npoly):\n",
    "        feature_matrix[:, idx] = scipy.special.eval_hermitenorm(idx, xdata) / compute_poly_norm(idx)\n",
    "    ydata = np.dot(feature_matrix, poly_coef)\n",
    "    ydata += noise_std * np.random.normal(0.0, 1.0, ndata)\n",
    "    return feature_matrix, ydata, poly_coef\n",
    "\n",
    "\n",
    "def sample_from_posterior_distribution(feature_matrix, ydata, sigma_pr, sigma_lh, nsample):\n",
    "    ndata, nfeat = feature_matrix.shape\n",
    "    hmat = np.dot(np.transpose(feature_matrix), feature_matrix) / ndata\n",
    "    hmat += sigma_lh * sigma_lh / sigma_pr / sigma_pr / ndata * np.eye(nfeat)\n",
    "    bvec = np.dot(ydata, feature_matrix) / ndata\n",
    "    cmean = np.linalg.solve(hmat, bvec)\n",
    "\n",
    "    eigh_val, eigh_vec = np.linalg.eigh(hmat)\n",
    "    eigh_val = sigma_lh / np.sqrt(ndata * eigh_val)\n",
    "    eigh_vec = np.transpose(eigh_vec)\n",
    "    dmat = np.zeros((nfeat, nfeat))\n",
    "    for idx in range(nfeat):\n",
    "        dmat[idx, idx] = eigh_val[idx]\n",
    "    csample = np.random.normal(0.0, 1.0, (nsample, nfeat))\n",
    "    csample = np.dot(csample, dmat)\n",
    "    csample = np.dot(csample, eigh_vec)\n",
    "    for idx in range(nsample):\n",
    "        csample[idx, :] += cmean\n",
    "    return csample\n",
    "\n",
    "\n",
    "def compute_log_likelihood_gradient(feature_matrix, ydata, cdata, sigma_lh):\n",
    "    ndata, nfeat = feature_matrix.shape\n",
    "    rdata = ydata - np.dot(feature_matrix, cdata)\n",
    "    mse = np.dot(rdata, rdata)\n",
    "    llh_grad = ndata / sigma_lh * (mse / sigma_lh / sigma_lh / ndata - 1.0)\n",
    "    return llh_grad\n",
    "\n",
    "\n",
    "def compute_log_evidence_grad(feature_matrix, ydata, sigma_pr, sigma_lh, num_sample):\n",
    "    csample = sample_from_posterior_distribution(feature_matrix, ydata, sigma_pr, sigma_lh, num_sample)\n",
    "    lev_grad = 0.0\n",
    "    for idx in range(num_sample):\n",
    "        lev_grad += compute_log_likelihood_gradient(feature_matrix, ydata, csample[idx, :], sigma_lh) / num_sample\n",
    "    return lev_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stochastic_gradient_descent(dim):\n",
    "    return np.random.normal(0.0, 1.0, dim)\n",
    "\n",
    "\n",
    "def single_step_stochastic_gradient_descent(\n",
    "    feature_matrix, ydata, num_samples,\n",
    "    s_k, dt, alpha, iteration_number\n",
    "):\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_optimium_stochastic_gradient_descent(\n",
    "    feature_matrix, ydata, num_samples, dt, alpha, num_iter\n",
    "):\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_s_param(dt, alpha, feature_matrix, ydata, num_samples, num_iter):\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem_4():\n",
    "    # prior sigma is always 1.0\n",
    "    dt_array = [0.001, 0.005, 0.0001, 0.0005, 0.0007, 0.00001, 0.00005]\n",
    "    alpha_array = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    npoly = 3\n",
    "    ndata = 1000\n",
    "    num_samples = 100\n",
    "    num_iter = 10000\n",
    "    feature_matrix, ydata, _ = make_regression_problem_4(npoly, ndata, noise_std=0.01)\n",
    "    result = []\n",
    "    for dt in tqdm(dt_array):\n",
    "        for alpha in alpha_array:\n",
    "            evidence_gradient, sigma, s = compute_s_param(dt, alpha, feature_matrix, ydata, num_samples, num_iter)\n",
    "\n",
    "    # How to choose alpha and dt?\n",
    "    # Your code here (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 5.\n",
    "\n",
    "При наличии метрического тензора (квадратичной формы, которая задана в каждой точке пространства) длина кривой может быть вычислена по формуле:\n",
    "$$\n",
    "l = \\int_{0}^{1} \\sqrt{\\frac{dx}{dt}^\\top g\\big(x(t)\\big) \\frac{dx}{dt}}\n",
    "$$\n",
    "\n",
    "Кривая, проходящая через две заданные точки, и имеющая наименьшую длину называется геодезической.\n",
    "\n",
    "Если кривая задана в виде ломаной, то длина кривой - функция координат вершин ломаной. При этом, если концевые точки зафиксированы, то можно геодезическую можно приближенно найти, варьирую координаты оставшихся звеньев ломаной.\n",
    "\n",
    "В данной задаче предлагается рассмотреть следующий метрический тензор, заданный в каждой точки плоскости:\n",
    "$$\n",
    "g(x) = \\frac{|x|^2 + 0.01}{|x|^2 + 1} I\n",
    "$$\n",
    "Задача - найти геодезическую, проходящую через точки $[-1, 0]$ и $[0.3, 0.1]$.\n",
    "\n",
    "Задачу предлагается решать, минимизирую длину кривой при помощи метода моментов (momentum method). Ниже представлен код, который решает данную задачу методом градиентного спуска.\n",
    "Модифицируйте его так, чтобы получился метод моментов. Постройте график с геодезической для случая $10$ и $50$ свободных вершин ломаной (параметр `nseg`). \n",
    "\n",
    "Как происходит инициализация? Что произойдет, если заменить инициализацию на случайную.\n",
    "\n",
    "Можно пользоваться следующим кодом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_tensor(x, r0=1.0):\n",
    "    eps = 1.0e-2\n",
    "    gmat = np.eye(x.size)\n",
    "    factor = (np.dot(x, x) + eps * r0 * r0) / (np.dot(x, x) + r0 * r0)\n",
    "    return factor * gmat\n",
    "\n",
    "\n",
    "def compute_segment_length(x0, x1, nsample=100):\n",
    "    t = np.linspace(0.0, 1.0, nsample + 1)\n",
    "    dx = (x1 - x0) / nsample\n",
    "    s = 0.0\n",
    "    for idx in range(nsample):\n",
    "        x = x0 + 0.5 * dx * (t[idx] + t[idx + 1])\n",
    "        gmat = compute_metric_tensor(x)\n",
    "        s += np.sqrt(np.dot(dx, np.dot(gmat, dx)))\n",
    "    return s\n",
    "\n",
    "\n",
    "def compute_pwl_curve_length(xpwl):\n",
    "    # length of piecewise linear curve\n",
    "    nseg, dimx = xpwl.shape\n",
    "    s = 0.0\n",
    "    for kseg in range(1, nseg):\n",
    "        s += compute_segment_length(xpwl[kseg - 1, :], xpwl[kseg, :])\n",
    "    return s\n",
    "\n",
    "\n",
    "def compute_objective_function(\n",
    "    x,\n",
    "    x0=np.asarray([-1.0, 0.0]),\n",
    "    x1=np.asarray([0.3, 0.1])\n",
    "):\n",
    "    dims = x0.size\n",
    "    nseg = np.int64(x.size / dims)\n",
    "    xdata = np.zeros((2 + nseg, dims))\n",
    "    xdata[0, :] = x0.copy()\n",
    "    xdata[-1, :] = x1.copy()\n",
    "    xdata[1: (-1), :] = np.reshape(x, (nseg, dims)).copy()\n",
    "    return compute_pwl_curve_length(xdata)\n",
    "\n",
    "\n",
    "def compute_objective_function_gradient(x):\n",
    "    dx = 1.0e-6\n",
    "    dimx = x.size\n",
    "    x0 = x.copy()\n",
    "    l0 = compute_objective_function(x0)\n",
    "    print('l0 = ' + str(l0))\n",
    "    grad = np.zeros(dimx)\n",
    "    for idx in range(dimx):\n",
    "        x1 = x0.copy()\n",
    "        x1[idx] += dx\n",
    "        l1 = compute_objective_function(x1)\n",
    "        grad[idx] = (l1 - l0) / dx\n",
    "    return grad\n",
    "\n",
    "\n",
    "def init_optimization_problem_5(\n",
    "    nseg,\n",
    "    x0=np.asarray([-1.0, 0.0]),\n",
    "    x1=np.asarray([0.3, 0.1])\n",
    "):\n",
    "    x = np.zeros((nseg, x0.size))\n",
    "    for kseg in range(nseg):\n",
    "        x[kseg, :] = (1 + kseg) / (nseg + 2) * x1 + (nseg - kseg + 1) / (nseg + 2) * x0\n",
    "    return x.flatten()\n",
    "\n",
    "\n",
    "def single_gradient_descent_step_problem_5(x, dt):\n",
    "    grad = compute_objective_function_gradient(x)\n",
    "    return x - dt * grad\n",
    "\n",
    "\n",
    "def compute_optimal_x_problem_5(nseg, dt, num_iter):\n",
    "    x0 = init_optimization_problem_5(nseg)\n",
    "    for _ in range(num_iter):\n",
    "        x1 = single_gradient_descent_step_problem_5(x0, dt)\n",
    "        x0 = x1.copy()\n",
    "    return x0\n",
    "\n",
    "\n",
    "def make_plot_problem_5():\n",
    "    nseg = 10\n",
    "    dt = 0.01\n",
    "    niter = 1000\n",
    "    x = compute_optimal_x_problem_5(nseg, dt, niter)\n",
    "    x = np.reshape(x, (nseg, -1))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([-1.0, x[0, 0]], [0.0, x[0, 1]], c='g')\n",
    "    for idx in range(1, x.shape[0]):\n",
    "        plt.plot([x[idx - 1, 0], x[idx, 0]], [x[idx - 1, 1], x[idx, 1]], c='b')\n",
    "    plt.plot([0.3, x[-1, 0]], [0.1, x[-1, 1]], c='r')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_plot_problem_5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
